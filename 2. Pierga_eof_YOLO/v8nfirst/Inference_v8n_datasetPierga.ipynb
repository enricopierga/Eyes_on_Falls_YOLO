{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ba6342",
   "metadata": {},
   "source": [
    "# Live inference webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44436333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¥ Webcam attiva. Premi 'q' per uscire.\n",
      "Classe: not_fallen, Confidenza: 0.73, BBox: [235.988525390625, 94.98905944824219, 544.9149780273438, 437.43267822265625]\n",
      "Classe: not_fallen, Confidenza: 0.72, BBox: [232.74542236328125, 95.60113525390625, 541.43359375, 429.9208984375]\n",
      "Classe: not_fallen, Confidenza: 0.72, BBox: [227.12786865234375, 7.0985107421875, 540.283203125, 480.0]\n",
      "Classe: not_fallen, Confidenza: 0.74, BBox: [234.4051513671875, 2.28070068359375, 546.1187133789062, 407.98211669921875]\n",
      "Classe: not_fallen, Confidenza: 0.72, BBox: [337.605712890625, 342.9576416015625, 465.2623291015625, 479.9127197265625]\n",
      "Classe: not_fallen, Confidenza: 0.73, BBox: [335.84033203125, 346.2034912109375, 464.6015625, 479.93048095703125]\n",
      "Classe: fallen, Confidenza: 0.71, BBox: [253.22691345214844, 19.179443359375, 561.9161376953125, 422.937744140625]\n",
      "Classe: fallen, Confidenza: 0.71, BBox: [257.0455322265625, 23.170318603515625, 567.2646484375, 426.2483825683594]\n",
      "Classe: fallen, Confidenza: 0.73, BBox: [237.73101806640625, 90.15927124023438, 554.217041015625, 424.4276428222656]\n",
      "Classe: fallen, Confidenza: 0.73, BBox: [237.73101806640625, 90.15927124023438, 554.217041015625, 424.4276428222656]\n",
      "Classe: fallen, Confidenza: 0.74, BBox: [236.4461212158203, 98.72134399414062, 553.2011108398438, 424.0554504394531]\n",
      "Classe: fallen, Confidenza: 0.74, BBox: [236.4461212158203, 98.72134399414062, 553.2011108398438, 424.0554504394531]\n",
      "Classe: fallen, Confidenza: 0.73, BBox: [238.55499267578125, 93.5023193359375, 555.2261352539062, 423.61370849609375]\n",
      "Classe: fallen, Confidenza: 0.71, BBox: [238.8712158203125, 87.76223754882812, 555.0252075195312, 421.4980773925781]\n",
      "Classe: fallen, Confidenza: 0.74, BBox: [272.8128662109375, 57.120147705078125, 585.5499267578125, 370.7748107910156]\n",
      "Classe: fallen, Confidenza: 0.70, BBox: [374.42755126953125, 61.89337158203125, 629.5634155273438, 323.8522033691406]\n",
      "Classe: fallen, Confidenza: 0.70, BBox: [374.42755126953125, 61.89337158203125, 629.5634155273438, 323.8522033691406]\n",
      "Classe: fallen, Confidenza: 0.74, BBox: [396.96124267578125, 18.008544921875, 640.0, 319.6700439453125]\n",
      "Classe: not_fallen, Confidenza: 0.71, BBox: [396.0634765625, 132.88888549804688, 639.296875, 480.0]\n",
      "Classe: fallen, Confidenza: 0.73, BBox: [0.0, 0.0, 264.6681213378906, 480.0]\n",
      "Classe: not_fallen, Confidenza: 0.77, BBox: [353.71258544921875, 57.15997314453125, 639.1593627929688, 443.24365234375]\n",
      "Classe: not_fallen, Confidenza: 0.73, BBox: [339.67523193359375, 69.55952453613281, 639.1115112304688, 445.09429931640625]\n",
      "Classe: not_fallen, Confidenza: 0.84, BBox: [0.39495849609375, 2.04669189453125, 260.68994140625, 480.0]\n",
      "Classe: not_fallen, Confidenza: 0.79, BBox: [14.354499816894531, 1.20867919921875, 264.1812744140625, 480.0]\n",
      "Classe: fallen, Confidenza: 0.71, BBox: [474.47894287109375, 29.308349609375, 639.7091674804688, 280.32452392578125]\n",
      "Classe: fallen, Confidenza: 0.74, BBox: [471.5245361328125, 27.009521484375, 639.8692626953125, 309.0283203125]\n",
      "Classe: fallen, Confidenza: 0.74, BBox: [220.77601623535156, 61.99505615234375, 565.368408203125, 383.31121826171875]\n",
      "Classe: not_fallen, Confidenza: 0.76, BBox: [9.2816162109375, 0.66357421875, 366.2603454589844, 479.45550537109375]\n",
      "Classe: not_fallen, Confidenza: 0.70, BBox: [220.23194885253906, 170.80215454101562, 515.4925537109375, 475.1477966308594]\n",
      "Classe: not_fallen, Confidenza: 0.70, BBox: [220.23194885253906, 170.80215454101562, 515.4925537109375, 475.1477966308594]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('v8n_dataset_pierga.pt')\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"âŒ Errore nell'aprire la webcam\")\n",
    "else:\n",
    "    print(\"ðŸŽ¥ Webcam attiva. Premi 'q' per uscire.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model.predict(frame, imgsz=640, conf=0.7, verbose=False)[0]\n",
    "    annotated_frame = results.plot()\n",
    "\n",
    "    # ðŸ–¨ï¸ Log su terminale\n",
    "    for box in results.boxes:\n",
    "        cls_id = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        xyxy = box.xyxy[0].tolist()\n",
    "        print(f\"Classe: {model.names[cls_id]}, Confidenza: {conf:.2f}, BBox: {xyxy}\")\n",
    "\n",
    "    cv2.imshow(\"YOLOv8 - Eyes On Falls LIVE\", annotated_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67da50d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.154  Python-3.11.9 torch-2.7.1+cpu CPU (Intel Core(TM) i7-8565U 1.80GHz)\n",
      "Model summary (fused): 72 layers, 3,006,038 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'v8n_dataset_pierga.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 6, 8400) (17.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.57...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.7s, saved as 'v8n_dataset_pierga.onnx' (11.7 MB)\n",
      "\n",
      "Export complete (3.4s)\n",
      "Results saved to \u001b[1mC:\\Users\\kikop\\Desktop\\EOF_YOLO\\Eyes_on_Falls_YOLO\\2. Pierga_eof_YOLO\\v8nfirst\u001b[0m\n",
      "Predict:         yolo predict task=detect model=v8n_dataset_pierga.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=v8n_dataset_pierga.onnx imgsz=640 data=data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'v8n_dataset_pierga.onnx'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Carichiamo il modello YOLOv11n addestrato\n",
    "model = YOLO('v8n_dataset_pierga.pt')  # qui metti il tuo path .pt\n",
    "\n",
    "# Esporta in ONNX (dinamico o fisso, in base alle esigenze NCNN)\n",
    "model.export(format='onnx', opset=12, dynamic=False, imgsz=640)\n",
    "\n",
    "# Otterrai: best.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009af1b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_runtime'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtflite_runtime\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minterpreter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtflite\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Configurazione\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tflite_runtime'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import time\n",
    "\n",
    "# Configurazione\n",
    "MODEL_PATH = \"v8n_dataset_pierga_float16.tflite\"\n",
    "INPUT_SIZE = 640\n",
    "CONF_THRESHOLD = 0.4\n",
    "IOU_THRESHOLD = 0.45\n",
    "CLASS_NAMES = [\"fallen\", \"not_fallen\"]\n",
    "\n",
    "# Load model\n",
    "interpreter = tflite.Interpreter(model_path=MODEL_PATH)\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = np.copy(x)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2  # x1\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2  # y1\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2  # x2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2  # y2\n",
    "    return y\n",
    "\n",
    "def process_output(output):\n",
    "    output = np.squeeze(output).transpose()  # (8400, 6)\n",
    "    boxes = sigmoid(output[:, 0:4])\n",
    "    scores = sigmoid(output[:, 4])\n",
    "    class_scores = sigmoid(output[:, 5:])\n",
    "    class_ids = np.argmax(class_scores, axis=-1)\n",
    "    class_conf = np.max(class_scores, axis=-1)\n",
    "    final_scores = scores * class_conf\n",
    "    mask = final_scores > CONF_THRESHOLD\n",
    "    boxes, final_scores, class_ids = boxes[mask], final_scores[mask], class_ids[mask]\n",
    "    boxes = xywh2xyxy(boxes) * INPUT_SIZE\n",
    "    return boxes, final_scores, class_ids\n",
    "\n",
    "# Video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, INPUT_SIZE)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, INPUT_SIZE)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    img_resized = cv2.resize(frame, (INPUT_SIZE, INPUT_SIZE))\n",
    "    input_data = np.expand_dims(img_resized, axis=0).astype(np.float32) / 255.0\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "    start = time.time()\n",
    "    interpreter.invoke()\n",
    "    end = time.time()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    boxes, scores, class_ids = process_output(output_data)\n",
    "\n",
    "    scale_x = frame.shape[1] / INPUT_SIZE\n",
    "    scale_y = frame.shape[0] / INPUT_SIZE\n",
    "\n",
    "    for i in range(len(boxes)):\n",
    "        x1, y1, x2, y2 = boxes[i]\n",
    "        x1, y1, x2, y2 = int(x1 * scale_x), int(y1 * scale_y), int(x2 * scale_x), int(y2 * scale_y)\n",
    "        label = f\"{CLASS_NAMES[class_ids[i]]}: {scores[i]:.2f}\"\n",
    "        color = (0, 255, 0) if class_ids[i] == 0 else (0, 0, 255)\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    fps = 1 / (end - start)\n",
    "    cv2.putText(frame, f\"FPS: {fps:.2f}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('YOLOv8 TFLite Live', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
