{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # 🏆 Confronto Prestazioni Modelli YOLO Pre-Addestrati\n",
    "# \n",
    "# Questo notebook confronta le prestazioni di 3 modelli YOLO già addestrati per il riconoscimento delle cadute.\n",
    "# \n",
    "# ## 📋 Modelli da confrontare:\n",
    "# 1. **Modello 1** - YOLOv11n \n",
    "# 2. **Modello 2** - YOLOv11s\n",
    "# 3. **Modello 3** - YOLOv11m o versione custom\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup e Import\n",
    "\n",
    "# %%\n",
    "# Installazione pacchetti necessari\n",
    "!pip install ultralytics>=8.1.0\n",
    "!pip install pandas numpy matplotlib seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install tabulate\n",
    "!pip install tqdm\n",
    "\n",
    "# %%\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from ultralytics import YOLO\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from tabulate import tabulate\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurazione matplotlib\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60699cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Configurazione\n",
    "\n",
    "# %%\n",
    "# MODIFICA QUESTI PATH CON I TUOI\n",
    "CONFIG = {\n",
    "    'models': {\n",
    "        'model1_the_hystory': '/Users/andreavisi/Desktop/PYTHON/Sistemi operativi Dedicati 2025/PROGETTO/Eyes_on_falls_YOLO/1. fallen_people_detection/fpds_minimal/weights/best.pt',  # MODIFICA\n",
    "        'model2_kiko_è_nano': '/Users/andreavisi/Desktop/PYTHON/Sistemi operativi Dedicati 2025/PROGETTO/Eyes_on_falls_YOLO/2. Pierga_eof_YOLO/v8nfirst/v8n_dataset_pierga.pt',  # MODIFICA\n",
    "        'thelastdance_11': '/Users/andreavisi/Desktop/PYTHON/Sistemi operativi Dedicati 2025/PROGETTO/Eyes_on_falls_YOLO/3. fallen_people_detection/best_fall_detection_yolo11.pt',  # MODIFICA\n",
    "    },\n",
    "    'test_data': {\n",
    "        'images_path': '/Users/andreavisi/Desktop/PYTHON/Sistemi operativi Dedicati 2025/PROGETTO/Eyes_on_falls_YOLO/Datasets/augmented_yolo_dataset/images/test',  # MODIFICA\n",
    "        'labels_path': '/Users/andreavisi/Desktop/PYTHON/Sistemi operativi Dedicati 2025/PROGETTO/Eyes_on_falls_YOLO/Datasets/augmented_yolo_dataset/labels/test',  # MODIFICA\n",
    "    },\n",
    "    'evaluation_params': {\n",
    "        'conf_threshold': 0.25,\n",
    "        'iou_threshold': 0.45,\n",
    "        'batch_size': 16,\n",
    "        'device': 'cpu'\n",
    "    },\n",
    "    'class_names': {\n",
    "        0: 'not_fallen',\n",
    "        1: 'fallen'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Directory per salvare i risultati\n",
    "RESULTS_DIR = Path('model_evaluation_results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"✅ Configurazione completata\")\n",
    "print(f\"📊 Modelli da confrontare: {list(CONFIG['models'].keys())}\")\n",
    "print(f\"📁 Test images: {CONFIG['test_data']['images_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c123e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Funzioni di Valutazione\n",
    "\n",
    "# %%\n",
    "def evaluate_model_performance(model_name, model_path, test_images_path, test_labels_path, config):\n",
    "    \"\"\"Valuta completamente un modello pre-addestrato\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"📊 Valutando: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'model_path': model_path\n",
    "    }\n",
    "    \n",
    "    # Verifica che il modello esista\n",
    "    if not Path(model_path).exists():\n",
    "        print(f\"❌ Modello non trovato: {model_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Carica modello\n",
    "    print(f\"📦 Caricando modello...\")\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # 1. Test su tutto il dataset\n",
    "    print(f\"🔍 Valutazione su test set...\")\n",
    "    test_images = list(Path(test_images_path).glob('*.jpg'))\n",
    "    test_labels = Path(test_labels_path)\n",
    "    \n",
    "    # Inizializza metriche\n",
    "    all_predictions = []\n",
    "    all_ground_truths = []\n",
    "    all_confidences = []\n",
    "    inference_times = []\n",
    "    \n",
    "    # Progress bar\n",
    "    for img_path in tqdm(test_images, desc=\"Processing images\"):\n",
    "        # Ground truth\n",
    "        label_path = test_labels / (img_path.stem + '.txt')\n",
    "        ground_truth = []\n",
    "        \n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        class_id = int(line.strip().split()[0])\n",
    "                        ground_truth.append(class_id)\n",
    "        \n",
    "        # Predizione con timing\n",
    "        start_time = time.time()\n",
    "        predictions = model.predict(\n",
    "            source=str(img_path),\n",
    "            conf=config['evaluation_params']['conf_threshold'],\n",
    "            iou=config['evaluation_params']['iou_threshold'],\n",
    "            device=config['evaluation_params']['device'],\n",
    "            verbose=False\n",
    "        )\n",
    "        inference_time = time.time() - start_time\n",
    "        inference_times.append(inference_time)\n",
    "        \n",
    "        # Estrai predizioni\n",
    "        pred_classes = []\n",
    "        pred_confidences = []\n",
    "        \n",
    "        for pred in predictions:\n",
    "            if pred.boxes is not None:\n",
    "                for box in pred.boxes:\n",
    "                    pred_classes.append(int(box.cls))\n",
    "                    pred_confidences.append(float(box.conf))\n",
    "        \n",
    "        # Salva per metriche (semplificato per demo)\n",
    "        if ground_truth and pred_classes:\n",
    "            all_ground_truths.append(ground_truth[0])\n",
    "            all_predictions.append(pred_classes[0])\n",
    "            all_confidences.append(pred_confidences[0])\n",
    "        elif ground_truth and not pred_classes:\n",
    "            all_ground_truths.append(ground_truth[0])\n",
    "            all_predictions.append(-1)  # Non rilevato\n",
    "            all_confidences.append(0.0)\n",
    "    \n",
    "    # 2. Calcola metriche\n",
    "    print(f\"\\n📈 Calcolando metriche...\")\n",
    "    \n",
    "    # Filtra non rilevati per metriche standard\n",
    "    valid_indices = [i for i, pred in enumerate(all_predictions) if pred != -1]\n",
    "    valid_gt = [all_ground_truths[i] for i in valid_indices]\n",
    "    valid_pred = [all_predictions[i] for i in valid_indices]\n",
    "    \n",
    "    # Classification report\n",
    "    if valid_gt and valid_pred:\n",
    "        report = classification_report(\n",
    "            valid_gt, \n",
    "            valid_pred,\n",
    "            target_names=['not_fallen', 'fallen'],\n",
    "            output_dict=True\n",
    "        )\n",
    "        \n",
    "        # Estrai metriche principali\n",
    "        results['metrics'] = {\n",
    "            'accuracy': report['accuracy'],\n",
    "            'precision_not_fallen': report['not_fallen']['precision'],\n",
    "            'recall_not_fallen': report['not_fallen']['recall'],\n",
    "            'f1_not_fallen': report['not_fallen']['f1-score'],\n",
    "            'precision_fallen': report['fallen']['precision'],\n",
    "            'recall_fallen': report['fallen']['recall'],\n",
    "            'f1_fallen': report['fallen']['f1-score'],\n",
    "            'macro_precision': report['macro avg']['precision'],\n",
    "            'macro_recall': report['macro avg']['recall'],\n",
    "            'macro_f1': report['macro avg']['f1-score']\n",
    "        }\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(valid_gt, valid_pred)\n",
    "        results['confusion_matrix'] = cm.tolist()\n",
    "    \n",
    "    # 3. Metriche di velocità\n",
    "    results['speed_metrics'] = {\n",
    "        'mean_inference_time': np.mean(inference_times),\n",
    "        'std_inference_time': np.std(inference_times),\n",
    "        'min_inference_time': np.min(inference_times),\n",
    "        'max_inference_time': np.max(inference_times),\n",
    "        'fps': 1.0 / np.mean(inference_times) if np.mean(inference_times) > 0 else 0,\n",
    "        'total_images': len(test_images),\n",
    "        'detection_rate': len(valid_indices) / len(all_ground_truths) if all_ground_truths else 0\n",
    "    }\n",
    "    \n",
    "    # 4. Dimensione modello\n",
    "    model_size_mb = Path(model_path).stat().st_size / (1024 * 1024)\n",
    "    results['model_size_mb'] = model_size_mb\n",
    "    \n",
    "    # 5. Analisi errori\n",
    "    false_negatives = sum(1 for pred in all_predictions if pred == -1)\n",
    "    results['error_analysis'] = {\n",
    "        'false_negative_count': false_negatives,\n",
    "        'false_negative_rate': false_negatives / len(all_predictions) if all_predictions else 0,\n",
    "        'total_predictions': len(all_predictions)\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ Valutazione completata per {model_name}\")\n",
    "    print(f\"   • Accuracy: {results['metrics']['accuracy']:.3f}\")\n",
    "    print(f\"   • FPS: {results['speed_metrics']['fps']:.2f}\")\n",
    "    print(f\"   • Model size: {model_size_mb:.1f} MB\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d233d148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Valutazione di Tutti i Modelli\n",
    "\n",
    "# %%\n",
    "# Valuta tutti i modelli\n",
    "all_results = {}\n",
    "\n",
    "for model_name, model_path in CONFIG['models'].items():\n",
    "    try:\n",
    "        results = evaluate_model_performance(\n",
    "            model_name=model_name,\n",
    "            model_path=model_path,\n",
    "            test_images_path=CONFIG['test_data']['images_path'],\n",
    "            test_labels_path=CONFIG['test_data']['labels_path'],\n",
    "            config=CONFIG\n",
    "        )\n",
    "        \n",
    "        if results:\n",
    "            all_results[model_name] = results\n",
    "            \n",
    "            # Salva risultati individuali\n",
    "            with open(RESULTS_DIR / f'{model_name}_evaluation.json', 'w') as f:\n",
    "                json.dump(results, f, indent=2)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Errore con {model_name}: {str(e)}\")\n",
    "        all_results[model_name] = {'error': str(e)}\n",
    "\n",
    "print(\"\\n✅ Valutazione completata per tutti i modelli!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ec4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Confronto Risultati\n",
    "\n",
    "# %%\n",
    "# Crea DataFrame comparativo\n",
    "comparison_data = []\n",
    "\n",
    "for model_name, results in all_results.items():\n",
    "    if 'error' not in results and 'metrics' in results:\n",
    "        row = {\n",
    "            'Model': model_name,\n",
    "            'Accuracy': results['metrics']['accuracy'],\n",
    "            'Precision (avg)': results['metrics']['macro_precision'],\n",
    "            'Recall (avg)': results['metrics']['macro_recall'],\n",
    "            'F1-Score (avg)': results['metrics']['macro_f1'],\n",
    "            'F1 not_fallen': results['metrics']['f1_not_fallen'],\n",
    "            'F1 fallen': results['metrics']['f1_fallen'],\n",
    "            'FPS': results['speed_metrics']['fps'],\n",
    "            'Inference Time (ms)': results['speed_metrics']['mean_inference_time'] * 1000,\n",
    "            'Model Size (MB)': results['model_size_mb'],\n",
    "            'Detection Rate': results['speed_metrics']['detection_rate']\n",
    "        }\n",
    "        comparison_data.append(row)\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Mostra tabella comparativa\n",
    "print(\"\\n📊 TABELLA COMPARATIVA MODELLI\")\n",
    "print(\"=\"*120)\n",
    "print(tabulate(df_comparison, headers='keys', tablefmt='grid', floatfmt=\".3f\"))\n",
    "\n",
    "# Salva CSV\n",
    "df_comparison.to_csv(RESULTS_DIR / 'model_comparison.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358f8ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Visualizzazioni Comparative\n",
    "\n",
    "# %%\n",
    "# Crea visualizzazioni\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Accuracy e F1-Score\n",
    "ax = axes[0, 0]\n",
    "metrics = ['Accuracy', 'F1-Score (avg)']\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.35\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = df_comparison[metric].values\n",
    "    ax.bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "ax.set_xlabel('Modelli')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Accuracy e F1-Score Medio', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width/2)\n",
    "ax.set_xticklabels(df_comparison['Model'], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. F1-Score per classe\n",
    "ax = axes[0, 1]\n",
    "metrics = ['F1 not_fallen', 'F1 fallen']\n",
    "x = np.arange(len(df_comparison))\n",
    "width = 0.35\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = df_comparison[metric].values\n",
    "    ax.bar(x + i*width, values, width, label=metric)\n",
    "\n",
    "ax.set_xlabel('Modelli')\n",
    "ax.set_ylabel('F1-Score')\n",
    "ax.set_title('F1-Score per Classe', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x + width/2)\n",
    "ax.set_xticklabels(df_comparison['Model'], rotation=45)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Velocità (FPS)\n",
    "ax = axes[0, 2]\n",
    "ax.bar(df_comparison['Model'], df_comparison['FPS'], color='green', alpha=0.7)\n",
    "ax.set_xlabel('Modelli')\n",
    "ax.set_ylabel('FPS')\n",
    "ax.set_title('Velocità Inferenza (FPS)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# 4. Trade-off: Accuracy vs Speed\n",
    "ax = axes[1, 0]\n",
    "scatter = ax.scatter(df_comparison['FPS'], \n",
    "                    df_comparison['Accuracy'], \n",
    "                    s=df_comparison['Model Size (MB)'] * 20,  # Dimensione proporzionale\n",
    "                    alpha=0.6,\n",
    "                    c=range(len(df_comparison)),\n",
    "                    cmap='viridis')\n",
    "\n",
    "for i, model in enumerate(df_comparison['Model']):\n",
    "    ax.annotate(model, \n",
    "                (df_comparison['FPS'].iloc[i], df_comparison['Accuracy'].iloc[i]),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "\n",
    "ax.set_xlabel('FPS')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Trade-off: Accuratezza vs Velocità\\n(dimensione cerchio = dimensione modello)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Tempo di inferenza\n",
    "ax = axes[1, 1]\n",
    "ax.bar(df_comparison['Model'], df_comparison['Inference Time (ms)'], color='orange', alpha=0.7)\n",
    "ax.set_xlabel('Modelli')\n",
    "ax.set_ylabel('Tempo (ms)')\n",
    "ax.set_title('Tempo Medio di Inferenza', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# 6. Detection Rate\n",
    "ax = axes[1, 2]\n",
    "ax.bar(df_comparison['Model'], df_comparison['Detection Rate'] * 100, color='purple', alpha=0.7)\n",
    "ax.set_xlabel('Modelli')\n",
    "ax.set_ylabel('Detection Rate (%)')\n",
    "ax.set_title('Tasso di Rilevamento', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'model_comparison_plots.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f9aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Confusion Matrix per Ogni Modello\n",
    "\n",
    "# %%\n",
    "# Visualizza confusion matrix per ogni modello\n",
    "fig, axes = plt.subplots(1, len(all_results), figsize=(6*len(all_results), 5))\n",
    "if len(all_results) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for idx, (model_name, results) in enumerate(all_results.items()):\n",
    "    if 'confusion_matrix' in results:\n",
    "        cm = np.array(results['confusion_matrix'])\n",
    "        \n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['not_fallen', 'fallen'],\n",
    "                   yticklabels=['not_fallen', 'fallen'],\n",
    "                   ax=axes[idx])\n",
    "        axes[idx].set_title(f'Confusion Matrix\\n{model_name}', fontweight='bold')\n",
    "        axes[idx].set_xlabel('Predicted')\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c08f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 8. Radar Chart Comparativo\n",
    "\n",
    "# %%\n",
    "# Prepara dati per radar chart\n",
    "categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'FPS_norm', 'Det_Rate']\n",
    "\n",
    "# Normalizza alcune metriche per il radar (0-1)\n",
    "df_radar = df_comparison.copy()\n",
    "df_radar['FPS_norm'] = df_radar['FPS'] / df_radar['FPS'].max()\n",
    "df_radar['Det_Rate'] = df_radar['Detection Rate']\n",
    "\n",
    "# Crea radar chart\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, projection='polar')\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(df_comparison)))\n",
    "\n",
    "for idx, row in df_radar.iterrows():\n",
    "    values = [\n",
    "        row['Accuracy'],\n",
    "        row['Precision (avg)'],\n",
    "        row['Recall (avg)'],\n",
    "        row['F1-Score (avg)'],\n",
    "        row['FPS_norm'],\n",
    "        row['Det_Rate']\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, \n",
    "            label=row['Model'], color=colors[idx])\n",
    "    ax.fill(angles, values, alpha=0.25, color=colors[idx])\n",
    "\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Confronto Multidimensionale dei Modelli', \n",
    "             fontsize=16, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(RESULTS_DIR / 'radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac809b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Analisi del Miglior Modello\n",
    "\n",
    "# %%\n",
    "# Determina il miglior modello secondo diversi criteri\n",
    "print(\"\\n🏆 ANALISI MIGLIOR MODELLO\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "criteria = {}\n",
    "\n",
    "# Calcola vincitori per ogni criterio\n",
    "if not df_comparison.empty:\n",
    "    criteria['Miglior Accuracy'] = df_comparison.loc[df_comparison['Accuracy'].idxmax(), 'Model']\n",
    "    criteria['Miglior F1-Score'] = df_comparison.loc[df_comparison['F1-Score (avg)'].idxmax(), 'Model']\n",
    "    criteria['Più Veloce (FPS)'] = df_comparison.loc[df_comparison['FPS'].idxmax(), 'Model']\n",
    "    criteria['Più Leggero'] = df_comparison.loc[df_comparison['Model Size (MB)'].idxmin(), 'Model']\n",
    "    criteria['Miglior Detection Rate'] = df_comparison.loc[df_comparison['Detection Rate'].idxmax(), 'Model']\n",
    "    \n",
    "    # Calcola un punteggio di trade-off complessivo\n",
    "    df_comparison['trade_off_score'] = (\n",
    "        df_comparison['Accuracy'] * 0.3 +\n",
    "        df_comparison['F1-Score (avg)'] * 0.3 +\n",
    "        (df_comparison['FPS'] / df_comparison['FPS'].max()) * 0.2 +\n",
    "        df_comparison['Detection Rate'] * 0.1 +\n",
    "        (1 - df_comparison['Model Size (MB)'] / df_comparison['Model Size (MB)'].max()) * 0.1\n",
    "    )\n",
    "    \n",
    "    criteria['Miglior Trade-off'] = df_comparison.loc[df_comparison['trade_off_score'].idxmax(), 'Model']\n",
    "    \n",
    "    # Stampa risultati\n",
    "    for criterio, vincitore in criteria.items():\n",
    "        print(f\"• {criterio}: {vincitore}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 10. Report Finale\n",
    "\n",
    "# %%\n",
    "# Genera report dettagliato\n",
    "report = f\"\"\"\n",
    "📋 REPORT VALUTAZIONE MODELLI YOLO - FALL DETECTION\n",
    "====================================================\n",
    "Data: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n",
    "\n",
    "1. MODELLI VALUTATI:\n",
    "--------------------\n",
    "\"\"\"\n",
    "\n",
    "for model in df_comparison['Model']:\n",
    "    report += f\"   • {model}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "2. RISULTATI PRINCIPALI:\n",
    "------------------------\n",
    "\"\"\"\n",
    "\n",
    "for _, row in df_comparison.iterrows():\n",
    "    report += f\"\\n   {row['Model']}:\\n\"\n",
    "    report += f\"   - Accuracy: {row['Accuracy']:.3f}\\n\"\n",
    "    report += f\"   - F1-Score medio: {row['F1-Score (avg)']:.3f}\\n\"\n",
    "    report += f\"   - F1 not_fallen: {row['F1 not_fallen']:.3f}\\n\"\n",
    "    report += f\"   - F1 fallen: {row['F1 fallen']:.3f}\\n\"\n",
    "    report += f\"   - FPS: {row['FPS']:.2f}\\n\"\n",
    "    report += f\"   - Tempo inferenza: {row['Inference Time (ms)']:.1f} ms\\n\"\n",
    "    report += f\"   - Detection Rate: {row['Detection Rate']*100:.1f}%\\n\"\n",
    "    report += f\"   - Dimensione: {row['Model Size (MB)']:.1f} MB\\n\"\n",
    "\n",
    "if criteria:\n",
    "    report += f\"\"\"\n",
    "3. VINCITORI PER CATEGORIA:\n",
    "---------------------------\n",
    "\"\"\"\n",
    "    for criterio, vincitore in criteria.items():\n",
    "        report += f\"   • {criterio}: {vincitore}\\n\"\n",
    "\n",
    "# Raccomandazioni\n",
    "report += f\"\"\"\n",
    "4. RACCOMANDAZIONI:\n",
    "-------------------\n",
    "\"\"\"\n",
    "\n",
    "if criteria:\n",
    "    report += f\"\"\"\n",
    "   📱 Per deployment real-time su CPU/Edge:\n",
    "      → Consigliato: {criteria.get('Più Veloce (FPS)', 'N/A')}\n",
    "      → Alternativa leggera: {criteria.get('Più Leggero', 'N/A')}\n",
    "   \n",
    "   🎯 Per massima accuratezza:\n",
    "      → Consigliato: {criteria.get('Miglior Accuracy', 'N/A')}\n",
    "      → Miglior F1-Score: {criteria.get('Miglior F1-Score', 'N/A')}\n",
    "   \n",
    "   ⚖️ Per uso bilanciato:\n",
    "      → Consigliato: {criteria.get('Miglior Trade-off', 'N/A')}\n",
    "\"\"\"\n",
    "\n",
    "# Salva report\n",
    "with open(RESULTS_DIR / 'evaluation_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77176fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 11. Test Visuale su Immagini Campione\n",
    "\n",
    "# %%\n",
    "def visual_comparison(models_dict, test_image_path, conf=0.25):\n",
    "    \"\"\"Confronto visuale delle predizioni dei modelli su una stessa immagine\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(models_dict) + 1, figsize=(5*(len(models_dict)+1), 5))\n",
    "    \n",
    "    # Mostra immagine originale\n",
    "    img = cv2.imread(test_image_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    axes[0].imshow(img_rgb)\n",
    "    axes[0].set_title('Immagine Originale', fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Predizioni per ogni modello\n",
    "    for idx, (model_name, model_path) in enumerate(models_dict.items()):\n",
    "        if Path(model_path).exists():\n",
    "            model = YOLO(model_path)\n",
    "            results = model.predict(source=test_image_path, conf=conf, save=False)\n",
    "            \n",
    "            # Visualizza risultato\n",
    "            if results and results[0].plot() is not None:\n",
    "                img_with_boxes = results[0].plot()\n",
    "                axes[idx+1].imshow(cv2.cvtColor(img_with_boxes, cv2.COLOR_BGR2RGB))\n",
    "            else:\n",
    "                axes[idx+1].imshow(img_rgb)\n",
    "                axes[idx+1].text(0.5, 0.5, 'Nessun rilevamento', \n",
    "                               ha='center', va='center', transform=axes[idx+1].transAxes)\n",
    "            \n",
    "            axes[idx+1].set_title(f'{model_name}', fontweight='bold')\n",
    "            axes[idx+1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Test su alcune immagini campione\n",
    "test_images = list(Path(CONFIG['test_data']['images_path']).glob('*.jpg'))[:3]\n",
    "\n",
    "for i, test_img in enumerate(test_images):\n",
    "    print(f\"\\n🖼️ Test visuale su: {test_img.name}\")\n",
    "    fig = visual_comparison(CONFIG['models'], str(test_img))\n",
    "    fig.savefig(RESULTS_DIR / f'visual_comparison_{i+1}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e44d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 12. Salvataggio Risultati Finali\n",
    "\n",
    "# %%\n",
    "# Salva tutti i risultati in un unico file JSON\n",
    "final_results = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'models_evaluated': list(CONFIG['models'].keys()),\n",
    "    'test_dataset': CONFIG['test_data'],\n",
    "    'comparison_table': df_comparison.to_dict('records'),\n",
    "    'detailed_results': all_results,\n",
    "    'winners': criteria if 'criteria' in locals() else {},\n",
    "    'config': CONFIG\n",
    "}\n",
    "\n",
    "with open(RESULTS_DIR / 'complete_evaluation.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\n✅ Tutti i risultati salvati in: {RESULTS_DIR}\")\n",
    "print(\"\\n📁 File generati:\")\n",
    "for file in sorted(RESULTS_DIR.glob('*')):\n",
    "    print(f\"   • {file.name}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 🎯 Conclusioni e Prossimi Passi\n",
    "# \n",
    "# Basandoti sui risultati:\n",
    "# \n",
    "# 1. **Scegli il modello più adatto** al tuo caso d'uso\n",
    "# 2. **Ottimizza ulteriormente** il modello scelto se necessario\n",
    "# 3. **Testa su nuovi dati** per validare le performance\n",
    "# 4. **Considera ensemble methods** se hai risorse sufficienti\n",
    "# \n",
    "# ### Quick Test Function\n",
    "\n",
    "# %%\n",
    "def quick_test(image_path, model_name=None):\n",
    "    \"\"\"Test rapido con il modello migliore o specificato\"\"\"\n",
    "    \n",
    "    if model_name is None:\n",
    "        # Usa il modello con miglior trade-off\n",
    "        model_name = criteria.get('Miglior Trade-off', list(CONFIG['models'].keys())[0])\n",
    "    \n",
    "    model_path = CONFIG['models'].get(model_name)\n",
    "    \n",
    "    if model_path and Path(model_path).exists():\n",
    "        print(f\"🔍 Testing con {model_name}\")\n",
    "        model = YOLO(model_path)\n",
    "        results = model.predict(source=image_path, conf=0.25, save=True)\n",
    "        \n",
    "        # Mostra risultati\n",
    "        for r in results:\n",
    "            if r.boxes is not None:\n",
    "                for box in r.boxes:\n",
    "                    cls = int(box.cls)\n",
    "                    conf = float(box.conf)\n",
    "                    print(f\"   • Rilevato: {CONFIG['class_names'][cls]} (conf: {conf:.2%})\")\n",
    "        \n",
    "        return results\n",
    "    else:\n",
    "        print(f\"❌ Modello {model_name} non trovato\")\n",
    "        return None\n",
    "\n",
    "# Esempio:\n",
    "# results = quick_test('path/to/new/image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320e49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
